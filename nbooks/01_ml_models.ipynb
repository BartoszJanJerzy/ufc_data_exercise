{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"D:\\PythonApps\\ufc_complete_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///tmp/mlflow_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"resources/df_features.csv\", index_col=0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "'fighter_hist_wins',\n",
    "'fighter_hist_looses', \n",
    "'fighter_hist_total',\n",
    "'fighter_title_fights', \n",
    "'wins_ratio', \n",
    "'tf_ratio',\n",
    "'wins_streak',\n",
    "'lost_streak',\n",
    "'SLpM_norm', \n",
    "'sig_str_acc_norm', \n",
    "'SApM_norm',\n",
    "'str_def_norm', \n",
    "'td_avg_norm', \n",
    "'td_acc_norm',\n",
    "'significant_strikes', \n",
    "'damage_defense', \n",
    "'offensive_grappling',\n",
    "'defensive_grappling', \n",
    "'submissions'\n",
    "]\n",
    "r_features = [f\"r_{x}\" for x in feature_cols]\n",
    "b_features = [f\"b_{x}\" for x in feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_data = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    diff_vector = []\n",
    "    for j, f in enumerate(feature_cols):\n",
    "        r = df.iloc[i][r_features[j]]\n",
    "        b = df.iloc[i][b_features[j]]\n",
    "        diff = float(r - b)\n",
    "        diff_vector.append(diff)\n",
    "    win = df.iloc[i][\"winner\"]\n",
    "    win_rank = 1 if df.iloc[i][\"winner\"] == \"Red\" else 0\n",
    "    diff_vector.append(win)\n",
    "    diff_vector.append(win_rank)\n",
    "    diff_data.append(diff_vector)\n",
    "\n",
    "diff_df = pd.DataFrame(\n",
    "    data=diff_data,\n",
    "    columns=feature_cols + [\"winner\", \"winner_rank\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete records with no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_empty_data = []\n",
    "for i in range(len(diff_df)):\n",
    "    row = diff_df.iloc[i][feature_cols].to_list()\n",
    "    is_empty = True\n",
    "    for x in row:\n",
    "        if bool(x):\n",
    "            is_empty = False\n",
    "            break\n",
    "    is_empty_data.append(is_empty)\n",
    "\n",
    "diff_df[\"is_empty\"] = is_empty_data\n",
    "diff_df = diff_df[diff_df[\"is_empty\"] == False].reset_index(drop=True).drop(\"is_empty\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "diff_df[feature_cols] = scaler.fit_transform(diff_df[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = diff_df[:-200]\n",
    "test_df = diff_df[-200:]\n",
    "\n",
    "\n",
    "x_train = train_df[feature_cols].to_numpy()\n",
    "x_test = test_df[feature_cols].to_numpy()\n",
    "y_train = train_df[\"winner_rank\"].to_numpy()\n",
    "y_test = test_df[\"winner_rank\"].to_numpy()\n",
    "\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = (\n",
    "    (\n",
    "        \"logistic_regression\", \n",
    "        LogisticRegression\n",
    "    ),\n",
    "    (\n",
    "        \"decision_tree\",\n",
    "        DecisionTreeClassifier\n",
    "    ),\n",
    "    (\n",
    "        \"random_forest\",\n",
    "        RandomForestClassifier\n",
    "    ),\n",
    "    (\n",
    "        \"gradient_boost\",\n",
    "        GradientBoostingClassifier\n",
    "    ),\n",
    "    (\n",
    "        \"xgboost\",\n",
    "        XGBClassifier\n",
    "    ),\n",
    "    (\n",
    "        \"lgbm\",\n",
    "        LGBMClassifier\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in models_dict:\n",
    "     with mlflow.start_run():\n",
    "        print()\n",
    "        print(\"-\"*100)\n",
    "        print(f'WORKING ON MODEL: {test[0]}')\n",
    "\n",
    "        model = test[1]()\n",
    "        model.fit(x_train, y_train)\n",
    "        train_acc = accuracy_score(y_train, model.predict(x_train))\n",
    "        test_acc = accuracy_score(y_test, model.predict(x_test))\n",
    "\n",
    "        mlflow.log_param(\"model\", test[0])\n",
    "        mlflow.log_metric(\"train_acc\", train_acc)\n",
    "        mlflow.log_metric(\"test_acc\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = mlflow.search_runs(experiment_ids=[\"0\"])[:len(models_dict)]\n",
    "runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "x_data = runs_df[\"params.model\"].to_list()\n",
    "y_data = runs_df[\"metrics.train_acc\"].to_list()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        name=\"Train\",\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        text=[round(y, 3) for y in y_data],\n",
    "        marker_color=\"teal\"\n",
    "    )\n",
    ")\n",
    "\n",
    "x_data = runs_df[\"params.model\"].to_list()\n",
    "y_data = runs_df[\"metrics.test_acc\"].to_list()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        name=\"Test\",\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        text=[round(y, 3) for y in y_data],\n",
    "        marker_color=\"orange\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=f\"<b>AUC PR values for tested models</b><br>Cross val reps = {cross_val_reps}\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    yaxis=dict(range=(0, 1.1))\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "loops = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_params() -> dict:\n",
    "    return dict(\n",
    "        num_leaves = np.random.randint(10, 251),\n",
    "        max_depth = np.random.randint(10, 501),\n",
    "        learning_rate = np.random.randint(1, 1000) / 1000,\n",
    "        n_estimators = np.random.randint(10, 501),\n",
    "        subsample = min(np.random.randint(90, 110) / 100, 1),\n",
    "        colsample_bytree = min(np.random.randint(90, 110) / 100, 1),\n",
    "        reg_alpha = np.random.randint(1, 100) / 1000,\n",
    "        reg_lambda = np.random.randint(1, 100) / 1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(loops)):\n",
    "    with mlflow.start_run():\n",
    "        params = return_params()\n",
    "        model = LGBMClassifier(**params, verbose=-1)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_acc = accuracy_score(y_train, model.predict(x_train))\n",
    "        test_acc = accuracy_score(y_test, model.predict(x_test))\n",
    "\n",
    "        mlflow.log_param(\"model\", f\"lgbm_tuning_{i}\")\n",
    "        for param, value in params.items():\n",
    "            mlflow.log_param(param, value)\n",
    "        mlflow.log_metric(\"train_acc\", train_acc)\n",
    "        mlflow.log_metric(\"test_acc\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "'run_id',\n",
    "'metrics.test_acc', \n",
    "'metrics.train_acc',\n",
    "'params.n_estimators',\n",
    "'params.reg_alpha', \n",
    "'params.subsample', \n",
    "'params.num_leaves',\n",
    "'params.max_depth', \n",
    "'params.colsample_bytree', \n",
    "'params.reg_lambda',\n",
    "'params.learning_rate', \n",
    "'params.model'\n",
    "]\n",
    "params_cols = [\n",
    "'params.n_estimators',\n",
    "'params.reg_alpha', \n",
    "'params.subsample', \n",
    "'params.num_leaves',\n",
    "'params.max_depth', \n",
    "'params.colsample_bytree', \n",
    "'params.reg_lambda',\n",
    "'params.learning_rate'\n",
    "]\n",
    "runs_df = mlflow.search_runs(experiment_ids=[\"0\"])[:loops][cols]\n",
    "runs_df[\"train_test_diff\"] = runs_df['metrics.train_acc'] - runs_df['metrics.test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in params_cols:\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=runs_df[col].astype(float),\n",
    "            y=runs_df[\"metrics.test_acc\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{col} x test acc\",\n",
    "        width=500,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df.sort_values([\"metrics.test_acc\", \"train_test_diff\"], ascending=[False, True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
