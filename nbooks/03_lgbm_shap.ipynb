{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"D:\\PythonApps\\ufc_complete_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv pip install -r requirements.txt --system\n",
    "!uv pip install matplotlib --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import mlflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"resources/df_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "'fighter_hist_wins',\n",
    "'fighter_hist_looses', \n",
    "'fighter_hist_total',\n",
    "'fighter_title_fights', \n",
    "'wins_ratio', \n",
    "'tf_ratio',\n",
    "'wins_streak',\n",
    "'lost_streak',\n",
    "'SLpM_norm', \n",
    "'sig_str_acc_norm', \n",
    "'SApM_norm',\n",
    "'str_def_norm', \n",
    "'td_avg_norm', \n",
    "'td_acc_norm',\n",
    "'significant_strikes', \n",
    "'damage_defense', \n",
    "'offensive_grappling',\n",
    "'defensive_grappling', \n",
    "'submissions'\n",
    "]\n",
    "r_features = [f\"r_{x}\" for x in feature_cols]\n",
    "b_features = [f\"b_{x}\" for x in feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_data = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    diff_vector = []\n",
    "    for j, f in enumerate(feature_cols):\n",
    "        r = df.iloc[i][r_features[j]]\n",
    "        b = df.iloc[i][b_features[j]]\n",
    "        diff = float(r - b)\n",
    "        diff_vector.append(diff)\n",
    "    win = df.iloc[i][\"winner\"]\n",
    "    win_rank = 1 if df.iloc[i][\"winner\"] == \"Red\" else 0\n",
    "    diff_vector.append(win)\n",
    "    diff_vector.append(win_rank)\n",
    "    diff_data.append(diff_vector)\n",
    "\n",
    "diff_df = pd.DataFrame(\n",
    "    data=diff_data,\n",
    "    columns=feature_cols + [\"winner\", \"winner_rank\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete records with no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_empty_data = []\n",
    "for i in range(len(diff_df)):\n",
    "    row = diff_df.iloc[i][feature_cols].to_list()\n",
    "    is_empty = True\n",
    "    for x in row:\n",
    "        if bool(x):\n",
    "            is_empty = False\n",
    "            break\n",
    "    is_empty_data.append(is_empty)\n",
    "\n",
    "diff_df[\"is_empty\"] = is_empty_data\n",
    "diff_df = diff_df[diff_df[\"is_empty\"] == False].reset_index(drop=True).drop(\"is_empty\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "diff_df[feature_cols] = scaler.fit_transform(diff_df[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = diff_df[:-200]\n",
    "test_df = diff_df[-200:]\n",
    "\n",
    "\n",
    "x_train = train_df[feature_cols].to_numpy()\n",
    "x_test = test_df[feature_cols].to_numpy()\n",
    "y_train = train_df[\"winner_rank\"].to_numpy()\n",
    "y_test = test_df[\"winner_rank\"].to_numpy()\n",
    "\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id = \"fbc3d4d9233e4dbe8b29b60d8154b0c0\"\n",
    "mlflow.set_tracking_uri(\"file:///tmp/mlflow_2\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[\"0\"])\n",
    "best_run = runs_df[runs_df[\"run_id\"] == mlflow_run_id].iloc[0]\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    num_leaves=int(best_run[\"params.num_leaves\"]),\n",
    "    max_depth=int(best_run[\"params.max_depth\"]),\n",
    "    learning_rate=float(best_run[\"params.learning_rate\"]),\n",
    "    n_estimators=int(best_run[\"params.n_estimators\"]),\n",
    "    subsample=float(best_run[\"params.subsample\"]),\n",
    "    colsample_bytree=float(best_run[\"params.colsample_bytree\"]),\n",
    "    reg_alpha=float(best_run[\"params.reg_alpha\"]),\n",
    "    reg_lambda=float(best_run[\"params.reg_lambda\"])\n",
    ")\n",
    "model = LGBMClassifier(**params, verbose=-1)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train_df[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
